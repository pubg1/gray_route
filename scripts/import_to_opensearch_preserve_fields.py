#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿ç•™æ‰€æœ‰åŸæœ‰å­—æ®µçš„ OpenSearch å¯¼å…¥è„šæœ¬
æŒ‰ç…§ README.md è®¾è®¡ï¼Œä¿ç•™ servicingcase_last.json çš„æ‰€æœ‰å­—æ®µï¼Œç‰¹åˆ«æ˜¯ä¸æ›´æ”¹æ•°æ® ID
"""

import json
import sys
import os
import logging
import re
from datetime import datetime
from opensearchpy import OpenSearch
from opensearch_config import OPENSEARCH_CONFIG, INDEX_CONFIG, IMPORT_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OpenSearchImporterPreserveFields:
    def __init__(self):
        """åˆå§‹åŒ– OpenSearch è¿æ¥"""
        try:
            # åˆ›å»º OpenSearch å®¢æˆ·ç«¯
            self.client = OpenSearch(
                hosts=[{
                    'host': OPENSEARCH_CONFIG['host'].replace('https://', '').replace('http://', ''),
                    'port': OPENSEARCH_CONFIG['port']
                }],
                http_auth=(OPENSEARCH_CONFIG['username'], OPENSEARCH_CONFIG['password']),
                use_ssl=OPENSEARCH_CONFIG['use_ssl'],
                verify_certs=OPENSEARCH_CONFIG['verify_certs'],
                ssl_assert_hostname=OPENSEARCH_CONFIG.get('ssl_assert_hostname', False),
                ssl_show_warn=OPENSEARCH_CONFIG.get('ssl_show_warn', False),
                timeout=OPENSEARCH_CONFIG.get('timeout', 30)
            )
            
            # æµ‹è¯•è¿æ¥
            info = self.client.info()
            logger.info(f"æˆåŠŸè¿æ¥åˆ° OpenSearch: {info['version']['number']}")
            
        except Exception as e:
            logger.error(f"è¿æ¥ OpenSearch å¤±è´¥: {e}")
            raise

    def clean_html_content(self, html_content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹ï¼Œæå–çº¯æ–‡æœ¬"""
        if not html_content:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        clean_text = re.sub(r'<[^>]+>', '', html_content)
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        # ç§»é™¤å›¾ç‰‡é“¾æ¥ç­‰æ— ç”¨ä¿¡æ¯
        clean_text = re.sub(r'https?://[^\s]+', '', clean_text)
        
        return clean_text

    def extract_phenomena_from_content(self, content: str, discussion: str) -> str:
        """ä»å†…å®¹ä¸­æå–æ•…éšœç°è±¡"""
        if not content:
            return discussion or ""
        
        # æ¸…ç†HTMLå†…å®¹
        clean_content = self.clean_html_content(content)
        
        # å°è¯•æå–æ•…éšœç°è±¡çš„å…³é”®ä¿¡æ¯
        # é€šå¸¸åœ¨å†…å®¹å¼€å¤´æˆ–åŒ…å«ç‰¹å®šå…³é”®è¯çš„éƒ¨åˆ†
        phenomena_keywords = [
            "æ•…éšœç°è±¡", "å®¢æˆ·åæ˜ ", "è½¦ä¸»æè¿°", "æ•…éšœæè¿°", 
            "ç—‡çŠ¶", "é—®é¢˜", "å¼‚å¸¸", "æ•…éšœ", "ä¸æ­£å¸¸"
        ]
        
        sentences = clean_content.split('ã€‚')
        phenomena_sentences = []
        
        for sentence in sentences[:5]:  # åªå–å‰5å¥
            sentence = sentence.strip()
            if len(sentence) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
                # å¦‚æœåŒ…å«æ•…éšœç°è±¡å…³é”®è¯ï¼Œä¼˜å…ˆé€‰æ‹©
                if any(keyword in sentence for keyword in phenomena_keywords):
                    phenomena_sentences.insert(0, sentence)
                else:
                    phenomena_sentences.append(sentence)
        
        # ç»„åˆæ•…éšœç°è±¡ï¼Œä¼˜å…ˆä½¿ç”¨discussion
        if discussion:
            phenomena = discussion
            if phenomena_sentences:
                phenomena += " / " + " / ".join(phenomena_sentences[:2])
        else:
            phenomena = " / ".join(phenomena_sentences[:3]) if phenomena_sentences else ""
        
        return phenomena[:500]  # é™åˆ¶é•¿åº¦

    def extract_system_and_part(self, content: str, discussion: str) -> tuple:
        """ä»å†…å®¹ä¸­æå–ç³»ç»Ÿå’Œéƒ¨ä»¶ä¿¡æ¯"""
        full_text = f"{discussion} {self.clean_html_content(content)}"
        
        # ç³»ç»Ÿæ˜ å°„
        system_keywords = {
            "å‘åŠ¨æœº": ["å‘åŠ¨æœº", "å¼•æ“", "ç‡ƒæ²¹", "ç‚¹ç«", "è¿›æ°”", "æ’æ°”", "å†·å´"],
            "å˜é€Ÿç®±/ä¼ åŠ¨": ["å˜é€Ÿå™¨", "å˜é€Ÿç®±", "ä¼ åŠ¨", "ç¦»åˆå™¨", "å·®é€Ÿå™¨"],
            "åˆ¶åŠ¨": ["åˆ¹è½¦", "åˆ¶åŠ¨", "ABS", "ESP", "åˆ¶åŠ¨è¸æ¿"],
            "è½¬å‘": ["è½¬å‘", "æ–¹å‘ç›˜", "åŠ©åŠ›", "è½¬å‘æŸ±"],
            "æ‚¬æŒ‚": ["æ‚¬æŒ‚", "å‡éœ‡", "å¼¹ç°§", "é¿éœ‡"],
            "ç”µå­ç”µæ°”": ["ç”µæ± ", "ç”µè·¯", "ECU", "ä¼ æ„Ÿå™¨", "çº¿æŸ", "æ¨¡å—"],
            "ç©ºè°ƒ": ["ç©ºè°ƒ", "åˆ¶å†·", "æš–é£", "å‹ç¼©æœº"],
            "è½¦èº«": ["è½¦é—¨", "è½¦çª—", "åº§æ¤…", "ç¯å…‰", "ä»ªè¡¨"],
            "å…¶ä»–": []
        }
        
        # æ£€æµ‹ç³»ç»Ÿ
        detected_system = "å…¶ä»–"
        for system, keywords in system_keywords.items():
            if any(keyword in full_text for keyword in keywords):
                detected_system = system
                break
        
        # æå–éƒ¨ä»¶ä¿¡æ¯ï¼ˆé€šå¸¸åœ¨discussionä¸­æˆ–å†…å®¹å¼€å¤´ï¼‰
        part_patterns = [
            r"([^ï¼Œã€‚]+(?:ä¼ æ„Ÿå™¨|æ¨¡å—|æ§åˆ¶å™¨|å¼€å…³|é˜€|æ³µ|ç®¡|çº¿æŸ|æ’å¤´|ç»§ç”µå™¨))",
            r"([^ï¼Œã€‚]+(?:ç³»ç»Ÿ|è£…ç½®|æœºæ„|æ€»æˆ))",
        ]
        
        detected_part = ""
        for pattern in part_patterns:
            matches = re.findall(pattern, full_text)
            if matches:
                detected_part = matches[0].strip()
                break
        
        if not detected_part and discussion:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šéƒ¨ä»¶ï¼Œä½¿ç”¨discussionä½œä¸ºéƒ¨ä»¶æè¿°
            detected_part = discussion[:50]
        
        return detected_system, detected_part

    def extract_tags(self, vehicletype: str, content: str, discussion: str) -> list:
        """æå–æ ‡ç­¾"""
        tags = []
        
        # æ·»åŠ è½¦å‹æ ‡ç­¾
        if vehicletype:
            tags.append(vehicletype)
        
        # æ·»åŠ ç»´ä¿®æ¡ˆä¾‹æ ‡ç­¾
        tags.append("ç»´ä¿®æ¡ˆä¾‹")
        
        # ä»å†…å®¹ä¸­æå–å…³é”®è¯ä½œä¸ºæ ‡ç­¾
        full_text = f"{discussion} {self.clean_html_content(content)}"
        
        # æ•…éšœç±»å‹æ ‡ç­¾
        fault_keywords = {
            "å¯åŠ¨æ•…éšœ": ["æ— æ³•å¯åŠ¨", "å¯åŠ¨å›°éš¾", "ä¸ç€è½¦"],
            "æ•…éšœç¯": ["æ•…éšœç¯", "è­¦å‘Šç¯", "æŠ¥è­¦"],
            "å¼‚å“": ["å¼‚å“", "å™ªéŸ³", "å£°éŸ³å¼‚å¸¸"],
            "æ¼æ²¹": ["æ¼æ²¹", "æ¸—æ²¹", "æ²¹æ¶²æ³„æ¼"],
            "è¿‡çƒ­": ["è¿‡çƒ­", "æ¸©åº¦é«˜", "é«˜æ¸©"],
            "æŒ¯åŠ¨": ["æŒ¯åŠ¨", "æŠ–åŠ¨", "ä¸ç¨³å®š"],
            "å¤±æ•ˆ": ["å¤±æ•ˆ", "ä¸å·¥ä½œ", "æ— æ•ˆæœ"]
        }
        
        for tag, keywords in fault_keywords.items():
            if any(keyword in full_text for keyword in keywords):
                tags.append(tag)
        
        return list(set(tags))  # å»é‡

    def transform_record(self, record: dict) -> dict:
        """è½¬æ¢å•æ¡è®°å½•ï¼Œä¿ç•™æ‰€æœ‰åŸæœ‰å­—æ®µ"""
        source = record.get('_source', {})
        
        # ä¿ç•™åŸæœ‰çš„æ‰€æœ‰å­—æ®µï¼ˆå®Œæ•´ä¿ç•™ï¼‰
        transformed = {}
        
        # ç›´æ¥å¤åˆ¶æ‰€æœ‰åŸæœ‰å­—æ®µ
        for key, value in source.items():
            transformed[key] = value
        
        # æ·»åŠ æŒ‰ç…§ README.md è®¾è®¡çš„æ–°å­—æ®µï¼Œç”¨äºæ•…éšœç°è±¡åŒ¹é…
        phenomena_text = self.extract_phenomena_from_content(
            source.get('search', ''), 
            source.get('discussion', '')
        )
        
        system, part = self.extract_system_and_part(
            source.get('search', ''), 
            source.get('discussion', '')
        )
        
        tags = self.extract_tags(
            source.get('vehicletype', ''),
            source.get('search', ''),
            source.get('discussion', '')
        )
        
        # æŒ‰ç…§ README.md çš„ JSONL æ ¼å¼æ·»åŠ å­—æ®µï¼ˆä¸è¦†ç›–åŸæœ‰å­—æ®µï¼‰
        # åªæ·»åŠ æ–°çš„åŒ¹é…å­—æ®µï¼Œå¦‚æœåŸå­—æ®µå­˜åœ¨åˆ™ä¸è¦†ç›–
        additional_fields = {
            # æ•…éšœç°è±¡åŒ¹é…ç›¸å…³å­—æ®µ
            'text': phenomena_text,  # ä¸»è¦çš„æ•…éšœç°è±¡æè¿°
            'system': system,        # ç³»ç»Ÿåˆ†ç±»
            'part': part,           # éƒ¨ä»¶ä¿¡æ¯
            'tags': tags,           # æ ‡ç­¾åˆ—è¡¨
            'popularity': source.get('searchNum', 0),  # ä½¿ç”¨searchNumä½œä¸ºçƒ­åº¦
            
            # æœç´¢ç›¸å…³å­—æ®µ
            'search_content': self.clean_html_content(source.get('search', '')),
            'import_time': datetime.now().isoformat(),
            
            # åŸå§‹æ•°æ®æ ‡è¯†
            'source_index': record.get('_index', ''),
            'source_type': record.get('_type', ''),
            'original_score': record.get('_score', 1)
        }
        
        # åªæ·»åŠ ä¸å­˜åœ¨çš„å­—æ®µï¼Œé¿å…è¦†ç›–åŸæœ‰æ•°æ®
        for key, value in additional_fields.items():
            if key not in transformed:
                transformed[key] = value
        
        return transformed

    def create_index_mapping(self, index_name: str):
        """åˆ›å»ºç´¢å¼•æ˜ å°„ï¼Œæ”¯æŒæ•…éšœç°è±¡åŒ¹é…"""
        try:
            if self.client.indices.exists(index=index_name):
                logger.info(f"ç´¢å¼• {index_name} å·²å­˜åœ¨")
                return True
            
            # æŒ‰ç…§ README.md è®¾è®¡çš„ç´¢å¼•æ˜ å°„ï¼ŒåŒ…å«æ‰€æœ‰åŸæœ‰å­—æ®µ
            mapping = {
                "mappings": {
                    "properties": {
                        # æ‰€æœ‰åŸæœ‰å­—æ®µçš„æ˜ å°„
                        "vehicletype": {
                            "type": "text",
                            "fields": {"keyword": {"type": "keyword"}}
                        },
                        "searchNum": {"type": "integer"},
                        "discussion": {"type": "text"},
                        "search": {"type": "text"},
                        "solution": {"type": "text"},
                        "rate": {"type": "float"},
                        "vin": {"type": "keyword"},
                        "id": {"type": "keyword"},
                        "summary": {"type": "text"},
                        "spare11": {"type": "text"},
                        "spare10": {"type": "text"},
                        "createtime": {"type": "date", "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"},
                        "faultcode": {"type": "text"},
                        "creatorid": {"type": "keyword"},
                        "spare4": {"type": "text"},
                        "spare3": {"type": "text"},
                        "spare6": {"type": "text"},
                        "spare5": {"type": "text"},
                        "spare15": {"type": "text"},
                        "egon": {"type": "text"},
                        "spare2": {"type": "text"},
                        "spare1": {"type": "text"},
                        "spare12": {"type": "text"},
                        "symptoms": {"type": "text"},
                        "money": {"type": "text"},
                        "vehiclebrand": {
                            "type": "text",
                            "fields": {"keyword": {"type": "keyword"}}
                        },
                        "casestate": {"type": "keyword"},
                        "topic": {"type": "text"},
                        "placement": {"type": "text"},
                        "noCode": {"type": "text"},
                        "searchContent": {"type": "text"},
                        
                        # æ•…éšœç°è±¡åŒ¹é…å­—æ®µï¼ˆæŒ‰ç…§ README.md è®¾è®¡ï¼‰
                        "text": {
                            "type": "text",
                            "analyzer": "standard"  # ä¸»è¦çš„æ•…éšœç°è±¡æ–‡æœ¬
                        },
                        "system": {
                            "type": "text",
                            "fields": {"keyword": {"type": "keyword"}}
                        },
                        "part": {
                            "type": "text",
                            "fields": {"keyword": {"type": "keyword"}}
                        },
                        "tags": {"type": "keyword"},
                        "popularity": {"type": "integer"},
                        
                        # æœç´¢è¾…åŠ©å­—æ®µ
                        "search_content": {"type": "text"},
                        "import_time": {"type": "date"},
                        
                        # åŸå§‹æ•°æ®å­—æ®µ
                        "source_index": {"type": "keyword"},
                        "source_type": {"type": "keyword"},
                        "original_score": {"type": "float"}
                    }
                }
            }
            
            response = self.client.indices.create(index=index_name, body=mapping)
            logger.info(f"æˆåŠŸåˆ›å»ºç´¢å¼•: {index_name}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            logger.info("å°è¯•è·³è¿‡ç´¢å¼•åˆ›å»ºï¼Œç›´æ¥å¯¼å…¥æ•°æ®...")
            return True

    def import_data(self, json_file: str, index_name: str, batch_size: int = 100):
        """å¯¼å…¥æ•°æ®åˆ° OpenSearchï¼Œä¿ç•™åŸæœ‰ID"""
        
        # åˆ›å»ºç´¢å¼•
        if not self.create_index_mapping(index_name):
            return False
        
        try:
            # è¯»å–JSONæ–‡ä»¶ï¼ˆæ”¯æŒJSONLæ ¼å¼ï¼‰
            logger.info(f"è¯»å–æ•°æ®æ–‡ä»¶: {json_file}")
            data = []
            
            with open(json_file, 'r', encoding='utf-8') as f:
                # å°è¯•è¯»å–ä¸ºæ ‡å‡†JSON
                content = f.read().strip()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
                if content.startswith('{') and '\n{' in content:
                    logger.info("æ£€æµ‹åˆ°JSONLæ ¼å¼ï¼ŒæŒ‰è¡Œè§£æ...")
                    f.seek(0)
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                record = json.loads(line)
                                data.append(record)
                            except json.JSONDecodeError as e:
                                logger.warning(f"ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")
                                continue
                else:
                    # å°è¯•ä½œä¸ºæ ‡å‡†JSONæ•°ç»„è§£æ
                    try:
                        data = json.loads(content)
                        if not isinstance(data, list):
                            logger.error("JSONæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåº”è¯¥æ˜¯æ•°ç»„æ ¼å¼æˆ–JSONLæ ¼å¼")
                            return False
                    except json.JSONDecodeError:
                        logger.error("æ–‡ä»¶æ—¢ä¸æ˜¯æ ‡å‡†JSONæ•°ç»„ä¹Ÿä¸æ˜¯JSONLæ ¼å¼")
                        return False
            
            logger.info(f"å…±è¯»å– {len(data)} æ¡è®°å½•")
            
            # è½¬æ¢æ•°æ®
            logger.info("å¼€å§‹è½¬æ¢æ•°æ®...")
            transformed_data = []
            
            for i, record in enumerate(data):
                try:
                    transformed = self.transform_record(record)
                    
                    # ä¿ç•™åŸæœ‰çš„ID
                    doc_id = record.get('_id')
                    if not doc_id:
                        logger.warning(f"è®°å½• {i} ç¼ºå°‘ _id å­—æ®µï¼Œè·³è¿‡")
                        continue
                    
                    transformed_data.append({
                        'id': doc_id,  # ä½¿ç”¨åŸæœ‰ID
                        'data': transformed
                    })
                    
                    if (i + 1) % 1000 == 0:
                        logger.info(f"å·²è½¬æ¢ {i + 1} æ¡è®°å½•")
                        
                except Exception as e:
                    logger.warning(f"è½¬æ¢è®°å½• {i} å¤±è´¥: {e}")
                    continue
            
            logger.info(f"æˆåŠŸè½¬æ¢ {len(transformed_data)} æ¡è®°å½•")
            
            if not transformed_data:
                logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥å¯¼å…¥")
                return False
            
            # æ‰¹é‡å¯¼å…¥
            logger.info("å¼€å§‹æ‰¹é‡å¯¼å…¥åˆ° OpenSearch...")
            success_count = 0
            error_count = 0
            
            for i in range(0, len(transformed_data), batch_size):
                batch = transformed_data[i:i + batch_size]
                
                # æ„å»ºæ‰¹é‡æ“ä½œ
                bulk_body = []
                for item in batch:
                    bulk_body.append({
                        "index": {
                            "_index": index_name,
                            "_id": item['id']  # ä½¿ç”¨åŸæœ‰ID
                        }
                    })
                    bulk_body.append(item['data'])
                
                try:
                    response = self.client.bulk(body=bulk_body)
                    
                    # æ£€æŸ¥ç»“æœ
                    for item in response['items']:
                        if 'index' in item:
                            if item['index']['status'] in [200, 201]:
                                success_count += 1
                            else:
                                error_count += 1
                                logger.warning(f"å¯¼å…¥å¤±è´¥: {item['index'].get('error', 'Unknown error')}")
                    
                    logger.info(f"å·²å¯¼å…¥ {min(i + batch_size, len(transformed_data))}/{len(transformed_data)} æ¡è®°å½•")
                    
                except Exception as e:
                    logger.error(f"æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}")
                    error_count += len(batch)
            
            logger.info(f"å¯¼å…¥å®Œæˆ: æˆåŠŸ {success_count} æ¡, å¤±è´¥ {error_count} æ¡")
            
            # åˆ·æ–°ç´¢å¼•
            self.client.indices.refresh(index=index_name)
            
            return success_count > 0
            
        except Exception as e:
            logger.error(f"å¯¼å…¥æ•°æ®å¤±è´¥: {e}")
            return False

    def search_phenomena(self, query: str, system: str = None, part: str = None, size: int = 10):
        """æŒ‰ç…§ README.md è®¾è®¡è¿›è¡Œæ•…éšœç°è±¡æœç´¢"""
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_body = {
                "query": {
                    "bool": {
                        "must": {
                            "multi_match": {
                                "query": query,
                                "fields": [
                                    "text^3",           # æ•…éšœç°è±¡æ–‡æœ¬æƒé‡æœ€é«˜
                                    "discussion^2",     # æ•…éšœç‚¹æè¿°æƒé‡æ¬¡ä¹‹
                                    "search_content^1", # å®Œæ•´å†…å®¹æƒé‡æœ€ä½
                                    "vehicletype^1.5"   # è½¦å‹ä¿¡æ¯
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        },
                        "filter": []
                    }
                },
                "size": size,
                "highlight": {
                    "fields": {
                        "text": {"fragment_size": 150, "number_of_fragments": 1},
                        "discussion": {"fragment_size": 100, "number_of_fragments": 1}
                    }
                },
                "sort": [
                    {"_score": {"order": "desc"}},
                    {"popularity": {"order": "desc"}}
                ]
            }
            
            # æ·»åŠ ç³»ç»Ÿè¿‡æ»¤
            if system:
                search_body["query"]["bool"]["filter"].append({
                    "term": {"system.keyword": system}
                })
            
            # æ·»åŠ éƒ¨ä»¶è¿‡æ»¤
            if part:
                search_body["query"]["bool"]["filter"].append({
                    "match": {"part": part}
                })
            
            response = self.client.search(index=INDEX_CONFIG['name'], body=search_body)
            
            results = []
            for hit in response['hits']['hits']:
                source = hit['_source']
                result = {
                    "id": hit['_id'],
                    "text": source.get('text', ''),
                    "system": source.get('system', ''),
                    "part": source.get('part', ''),
                    "tags": source.get('tags', []),
                    "vehicletype": source.get('vehicletype', ''),
                    "popularity": source.get('popularity', 0),
                    "score": hit['_score'],
                    "highlight": hit.get('highlight', {})
                }
                results.append(result)
            
            return {
                "query": query,
                "total": response['hits']['total']['value'],
                "results": results
            }
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return {"query": query, "total": 0, "results": []}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯¼å…¥ servicingcase_last.json åˆ° OpenSearch")
    print("ä¿ç•™æ‰€æœ‰åŸæœ‰å­—æ®µï¼Œæ”¯æŒæ•…éšœç°è±¡åŒ¹é…")
    print("=" * 60)
    
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_file = os.path.join(os.path.dirname(__file__), '../data/servicingcase_last.json')
    
    if not os.path.exists(data_file):
        logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return False
    
    logger.info(f"æ•°æ®æ–‡ä»¶: {data_file}")
    logger.info(f"ç›®æ ‡ç´¢å¼•: {INDEX_CONFIG['name']}")
    logger.info(f"OpenSearch: {OPENSEARCH_CONFIG['host']}:{OPENSEARCH_CONFIG['port']}")
    
    try:
        # åˆ›å»ºå¯¼å…¥å™¨
        importer = OpenSearchImporterPreserveFields()
        
        # å¯¼å…¥æ•°æ®
        success = importer.import_data(
            json_file=data_file,
            index_name=INDEX_CONFIG['name'],
            batch_size=IMPORT_CONFIG['batch_size']
        )
        
        if success:
            print("\nğŸ‰ æ•°æ®å¯¼å…¥æˆåŠŸ!")
            
            # è¿›è¡Œæ•…éšœç°è±¡æœç´¢æµ‹è¯•
            print("\nğŸ” è¿›è¡Œæ•…éšœç°è±¡æœç´¢æµ‹è¯•...")
            test_queries = [
                {"query": "å‘åŠ¨æœºæ— æ³•å¯åŠ¨", "system": "å‘åŠ¨æœº"},
                {"query": "åˆ¹è½¦å‘è½¯", "system": "åˆ¶åŠ¨"},
                {"query": "å˜é€Ÿå™¨æŒ‚æ¡£å†²å‡»", "system": "å˜é€Ÿç®±/ä¼ åŠ¨"},
                {"query": "ç©ºè°ƒä¸åˆ¶å†·", "system": "ç©ºè°ƒ"}
            ]
            
            for test in test_queries:
                print(f"\næµ‹è¯•æŸ¥è¯¢: {test['query']} (ç³»ç»Ÿ: {test.get('system', 'å…¨éƒ¨')})")
                results = importer.search_phenomena(
                    query=test['query'],
                    system=test.get('system'),
                    size=3
                )
                
                print(f"æ‰¾åˆ° {results['total']} ä¸ªç›¸å…³æ¡ˆä¾‹:")
                for i, result in enumerate(results['results'], 1):
                    print(f"  {i}. [{result['id']}] {result['text'][:100]}...")
                    print(f"     è½¦å‹: {result['vehicletype']}, ç³»ç»Ÿ: {result['system']}")
                    print(f"     è¯„åˆ†: {result['score']:.2f}, çƒ­åº¦: {result['popularity']}")
            
            print(f"\nâœ… å¯¼å…¥å®Œæˆ! ç´¢å¼•åç§°: {INDEX_CONFIG['name']}")
            print(f"ğŸ’¡ å¯ä»¥ä½¿ç”¨ /match API è¿›è¡Œæ•…éšœç°è±¡åŒ¹é…æŸ¥è¯¢")
            return True
        else:
            print("\nâŒ æ•°æ®å¯¼å…¥å¤±è´¥!")
            return False
            
    except Exception as e:
        logger.error(f"å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
